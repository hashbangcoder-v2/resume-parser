# Development Overrides
# This is the development configuration for the backend.

app:
  env: "dev"
  host: "0.0.0.0" # Listen on all interfaces for remote access
  port: 8000
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
    - "*" # Allow all for flexible dev

vllm:
  inference_args:
    gpu_memory_utilization: 0.65  # Conservative for A100 80GB with multimodal model
    max_model_len: 32768  # Increase to handle multimodal tokens (warning said 32768 needed)
    enforce_eager: True  # Use eager mode for better multimodal compatibility  
    tensor_parallel_size: 1
    trust_remote_code: True    
    limit_mm_per_prompt: {"image": 3}  
    repetition_penalty: 1.1
    temperature: 0.7

  env_vars:
    PYTORCH_CUDA_ALLOC_CONF: 'expandable_segments:True,max_split_size_mb:512'
    CUDA_VISIBLE_DEVICES: 0
    CUDA_LAUNCH_BLOCKING: 0
    TORCH_CUDA_ALLOC_SYNC_MIN_VERSION: 1
    
    VLLM_USE_TRITON_FLASH_ATTN: 'false'  # Enable flash attention for A100
    VLLM_WORKER_MULTIPROC_METHOD: 'fork'  # Use fork instead of spawn on Linux
    VLLM_ATTENTION_BACKEND: 'XFORMERS'  # Use standard Flash Attention for Qwen2.5-VL
    VLLM_NO_USAGE_STATS: 1
    VLLM_ENABLE_V1_MULTIMODAL: 'true'  # Enable multimodal support

  
logging:
  level: "DEBUG"
  file: "${oc.env:PROJECT_ROOT}/logs/backend_dev.log"

ai_model:
  default: "Qwen/Qwen2.5-Omni-7B"
  temperature: 0.2
  health_check_interval_seconds: 30
  health_check_timeout_seconds: 5  
  models:    
    - "Qwen/Qwen2.5-Omni-7B"
    - "qwen3"

local_storage:
  path: "${oc.env:PROJECT_ROOT}/uploaded_resumes" 

database:
  create_if_not_exists: true  
  directory: "${oc.env:PROJECT_ROOT}/database"
  url: "sqlite:///${database.directory}/dev_candidates.db"